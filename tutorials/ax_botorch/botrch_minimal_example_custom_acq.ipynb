{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Minimal example of custom acquisiton function\n",
    "This notebooks covers the consideration for setting up a Custom Acquisitions function, and optimising it with botorch and Ax.\n",
    "\n",
    "Problem description:\n",
    "- TODO: flesh this out: why do we not just use the gp to model objective = pdf * underling function in this example (and in general)\n",
    "\n",
    "This notebook covers:\n",
    "- Set up (through botorch):\n",
    "    - problem (true underling function and datapoint)\n",
    "    - Setup GP\n",
    "- Basic Acquisition function:\n",
    "    - `__init__` and `forward` inputs and outputs\n",
    "    - Behaviour expected by the things that then optimise this.\n",
    "- Optimisation of Acqusitions function (through Botorch):\n",
    "    - Brute force scoring of acquisiton function over the domain\n",
    "    - High level botorch automatic tool `optimise_acqf` (builds on the bellow)\n",
    "    - lower level botorch (`gen_batch_initial_conditions`, `gen_candidates_scipy`)\n",
    "    - Direct `scipy` optimisation\n",
    "- Multiple rounds of DOE (gp + acquisition -> pick new point and train new gp)\n",
    "    - TODO: what is the true optimal of the problem\n",
    "    - refit hyperparams every step (and condition)\n",
    "    - don't refit hyperparms each step (just condition)\n",
    "- Put into `Ax` framework.\n",
    "    - Set up the `acqf_input_constructor` required\n",
    "    - ax.BotorchModel using custom acquisition and acquisition args (all wraped in TorchModelBridge)\n",
    "    - Wrapping above into a `GenerationStratergy`\n",
    "- Exploration\n",
    "    - WIP: Supporting q = 2 optimisation, using sequential q=1. This is similar to lookahead\n",
    "    - Differntiation through 2 levels of fantasy GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "from ax.core import (\n",
    "    Arm,\n",
    "    Data,\n",
    "    Experiment,\n",
    "    Metric,\n",
    "    Objective,\n",
    "    OptimizationConfig,\n",
    "    ParameterType,\n",
    "    RangeParameter,\n",
    "    Runner,\n",
    "    SearchSpace,\n",
    ")\n",
    "from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n",
    "from ax.modelbridge.registry import Models\n",
    "from ax.plot.slice import plot_slice\n",
    "from ax.utils.common.result import Ok\n",
    "from ax.utils.notebook.plotting import render\n",
    "from botorch.acquisition import input_constructors\n",
    "from botorch.acquisition.acquisition import AcquisitionFunction\n",
    "from botorch.acquisition.input_constructors import acqf_input_constructor\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.generation.gen import gen_candidates_scipy\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.model import Model\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.optim.initializers import gen_batch_initial_conditions\n",
    "from botorch.posteriors.gpytorch import GPyTorchPosterior\n",
    "from botorch.utils.transforms import t_batch_mode_transform\n",
    "from matplotlib.axes import Axes\n",
    "from torch.distributions.distribution import Distribution\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")  # \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# As explained here there are a lot of issues that come up when using dtype = float32 (This is the default for torch\n",
    "# because its faster than float64) .Specifically this will cause issues with scipy optimise, because it will recommends\n",
    "# a small step (.4 ->.40000000001), but in the conversion to float34 it will be truncated back to .4, and the model will\n",
    "#  produce the exact same output\n",
    "# https://github.com/pytorch/botorch/discussions/1444\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the true underlying function\n",
    "This is the find we model with our GP (e.g the simulator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def underling_function(x: torch.Tensor) -> torch.Tensor:\n",
    "    return -3 * torch.sin(3 * x) - x**2 + 0.7 * x + 3\n",
    "\n",
    "\n",
    "x = torch.linspace(0, 1, 100)\n",
    "y_true = underling_function(x)\n",
    "\n",
    "_ = plt.plot(x.numpy(), y_true.numpy())\n",
    "_ = plt.title(\"true function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = torch.tensor([[0.0], [1.0]])  # 1D search space [lower_bound, upper_bound]\n",
    "random_initial_points = 2\n",
    "\n",
    "train_x = torch.tensor([[0.1], [0.85]])\n",
    "train_y = underling_function(train_x)\n",
    "\n",
    "\n",
    "_ = plt.plot(x.numpy(), y_true.numpy())\n",
    "_ = plt.scatter(train_x.numpy(), train_y.numpy(), c=\"red\")\n",
    "_ = plt.title(\"true function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single task GP: Single independant output, with homoskedastic noise (fit from data)\n",
    "# Note: this model can be used as is, but the hyperparams have not yet been fit for this specific problem\n",
    "gp = SingleTaskGP(train_X=train_x, train_Y=train_y)\n",
    "\n",
    "print(gp)\n",
    "\n",
    "# mll: marginal log likelihood. This can be thought of as the \"loss\" function which is thin optimised\n",
    "# with `fit_gpytorch_model(mll)` to find the hyperparameters (e.g length_scale etc).\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "_ = fit_gpytorch_model(mll);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the result of the fitted Gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like gypytorch, model should be set to eval prior to making predictions\n",
    "_ = gp.eval()\n",
    "# The Likelihood also needs to be set to eval if its not automatically within the model likelihood.eval()\n",
    "\n",
    "\n",
    "# Helper\n",
    "def plot_gp(model: Model, X: torch.Tensor | None = None, posterior_samples: int = 10, ax: None | Axes = None) -> Axes:  # noqa: N803\n",
    "    \"\"\"Plots each of the model targets over the domain. Also plots the training data.\n",
    "\n",
    "    Args:\n",
    "        model: assumed to be relevant to this demo problem (targets, domain etc).\n",
    "            # TODO(sw 2024-11-8): currently only testing with SingleTaskGP\n",
    "        X: (n,1): Linspace of [0,1] is used by default. Only 1d is currently supported.\n",
    "        posterior_samples: Number of posterior samples to draw and plot\n",
    "        ax: will plot to this axis if provied\n",
    "    \"\"\"\n",
    "    X = X if (X is not None) else torch.linspace(0, 1, 201).reshape(-1, 1)  # noqa: N806\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    # Quick and dirty way to ensure consistent colours\n",
    "    colours = [\"b\", \"c\", \"m\", \"y\", \"k\", \"r\"]\n",
    "\n",
    "    train_x_all = model.train_inputs[0].detach()\n",
    "    train_y_all = model.train_targets.detach()\n",
    "    train_var_all = model.likelihood.noise.detach()\n",
    "\n",
    "    # t = 1 then:\n",
    "    # shape model.train_targets: (n)\n",
    "    # shape model.train_inputs: ((n,d),)\n",
    "    if len(model.train_targets.shape) == 1:\n",
    "        n_targets = 1\n",
    "        # need to add a t dimension to all of these so they can be treated the same way as the multicase\n",
    "        train_x_all = train_x_all.unsqueeze(0)\n",
    "        train_y_all = train_y_all.unsqueeze(0)\n",
    "        train_var_all = train_var_all.unsqueeze(0)\n",
    "\n",
    "    # t > 1 then:\n",
    "    # shape model.train_targets: (t,n)\n",
    "    # shape model.train_inputs: ((t,n,d),)\n",
    "    else:\n",
    "        n_targets = model.train_targets.shape[-1]\n",
    "\n",
    "    for target_idx in range(n_targets):\n",
    "        c = colours[target_idx % len(colours)]\n",
    "\n",
    "        train_x = train_x_all[target_idx]\n",
    "        train_y = train_y_all[target_idx]\n",
    "        train_var = train_var_all[target_idx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            posterior: GPyTorchPosterior = model.posterior(X)  # type: ignore\n",
    "\n",
    "        mean = posterior.mean[:, target_idx]\n",
    "        var = posterior.variance[:, target_idx]\n",
    "        _ = ax.fill_between(X.flatten(), mean - 1.95 * var**0.5, mean + 1.95 * var**0.5, alpha=0.3, color=c)\n",
    "        _ = ax.plot(X, mean, color=c)\n",
    "        _ = ax.scatter(train_x.flatten(), train_y, color=c)\n",
    "        _ = ax.errorbar(train_x.flatten(), train_y, 1.95 * train_var**0.5, fmt=\"o\", color=c)\n",
    "\n",
    "        if posterior_samples > 0:\n",
    "            samples = posterior.rsample(torch.Size([posterior_samples]))[..., target_idx]\n",
    "            for sample in samples:\n",
    "                _ = ax.plot(X.cpu().numpy(), sample.cpu().numpy(), \"grey\", alpha=0.3)\n",
    "\n",
    "    _ = ax.set_title(\"Gp prediction\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "# test model on 101 regular spaced points on the interval [0, 1]\n",
    "_ = plot_gp(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define mock optimisation problem\n",
    "Typically, bayesian optimisation is about find the max of the true underling function.\n",
    "\n",
    "In axtreme, we need to integrated the GP over weather conditions, and we want to find areas of the GP that could reduce uncertainty in this calculation. This is not the same as standard ofptimisation of the GP, so a different approach is needed. This is discussed further in the axtreme package.\n",
    "\n",
    "Here we set up a more minimal example where:\n",
    "- We have a GP surrogating some underlying funtion\n",
    "- The next points that we want to chose if effected by a distribution as well as the GP\n",
    "\n",
    "While bayesian optimisation could be performed directly on the the objective function we define, the purpose of this is to use a GP and a simple distribution as a placeholder for more complex functionality, and show how CustomAcquisiton functions are integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST = Normal(0.3, 0.2)\n",
    "\n",
    "\n",
    "def objective_func(X: torch.Tensor) -> torch.Tensor:  # noqa: N803\n",
    "    prb = DIST.log_prob(X).exp()\n",
    "    y_true = underling_function(X)\n",
    "    return y_true * prb\n",
    "\n",
    "\n",
    "def plot_gp_and_dist(gp: Model, acquisiton_function: AcquisitionFunction | None = None, ax: Axes | None = None):\n",
    "    X = torch.linspace(0, 1, 201).reshape(-1, 1)  # noqa: N806\n",
    "\n",
    "    ax = plot_gp(gp, ax=ax)\n",
    "\n",
    "    # Add additional plots we find useful.\n",
    "    # The env dist\n",
    "    _ = ax.plot(X.numpy(), DIST.log_prob(X).exp(), color=\"green\", label=\"dist pdf\")\n",
    "    # true fucntion:\n",
    "    _ = ax.plot(X.numpy(), underling_function(X), color=\"red\", label=\"True underling\")\n",
    "    # The objectiove function\n",
    "    objective_results = objective_func(X)\n",
    "    x_max = X[objective_results.argmax()][0]\n",
    "    _ = ax.plot(\n",
    "        X,\n",
    "        objective_results,\n",
    "        color=\"black\",\n",
    "        label=f\"Objective function.\\nxmax {x_max:.3f}\\nscore {objective_results.max():.3f}\",\n",
    "    )\n",
    "    # formatting\n",
    "    _ = ax.legend(loc=\"upper right\")\n",
    "\n",
    "    ax2 = None\n",
    "    if acquisiton_function:\n",
    "        scores = acquisiton_function(X.reshape(-1, 1, 1))\n",
    "        ax2 = ax.twinx()\n",
    "        _ = ax2.plot(X.detach().numpy(), scores.detach().numpy(), label=\"ac score\", color=\"orange\")\n",
    "        _ = ax2.set_ylabel(\"ac_function_score\", color=\"orange\")\n",
    "        _ = ax2.tick_params(\"y\", color=\"orange\")\n",
    "\n",
    "    return ax2 or ax\n",
    "\n",
    "\n",
    "_ = plot_gp_and_dist(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_xmax_with_gp(model: Model):\n",
    "    test_x = torch.linspace(0, 1, 1001)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        posterior = model.posterior(test_x)\n",
    "\n",
    "    y_pred = posterior.mean.flatten()\n",
    "    prb = DIST.log_prob(test_x).exp()\n",
    "\n",
    "    objective = y_pred * prb\n",
    "\n",
    "    return test_x[objective.argmax()]\n",
    "\n",
    "\n",
    "gp_xmax_est = estimate_xmax_with_gp(gp)\n",
    "print(f\"gp xmax est {gp_xmax_est}. objective_score {objective_func(gp_xmax_est)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Acqusition function\n",
    "Define the custom acquisiton function that can be optimisted through `botorch` and `Ax`.\n",
    "\n",
    "Requirements from `botorch` are detailed here. Requirements from `Ax` are detailed section \"Putting into Ax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(sw 2024-11-13): need to check if this is the right type of class - there are more specific ones (e.g analytical)\n",
    "class CustomAcq(AcquisitionFunction):\n",
    "    \"A simple AcquisitionFunction for a 1d problem to demonstrate interface and integeraton.\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,  # Mandatory Param required by the interface\n",
    "        # Every thing below this point is option that we choose to inlucde\n",
    "        dist: Distribution,\n",
    "    ) -> None:\n",
    "        r\"\"\"Constructor for the AcquisitionFunction base class.\n",
    "\n",
    "        Args:\n",
    "            model: A fitted model.\n",
    "            dist: A distribtuion\n",
    "        \"\"\"\n",
    "        super().__init__(model=model)  # create the attribute self.model\n",
    "\n",
    "        self.dist: Distribution = dist\n",
    "\n",
    "        # Helper to show the hyperparams of the GP. This is useful to know if the GO is being refit between DOE rounds\n",
    "        # print(f\"\\nCustomAcq GP recieved:\\n{gp.likelihood.noise=}\\n{gp.covar_module.base_kernel.lengthscale=}\\n{gp.covar_module.outputscale=}\")  # noqa: E501, ERA001\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)  # Ensure function gets X.shape = (t,1,d)\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:  # type: ignore  # noqa: N803\n",
    "        r\"\"\"Evaluate the acquisition function on the candidate set X.\n",
    "\n",
    "        Args:\n",
    "            X: A `(b) x q x d`-dim Tensor of `(b)` t-batches with `q` `d`-dim\n",
    "                design points each. Loosely speaking, q-btaches and t-batchs translate to.\n",
    "                - `q-batch`: number of points output for a single DoE step. e.g q-batch=2, each time a new GP is trained\n",
    "                   in the DoE process, it has 2 additional points\n",
    "                - `t-batch`: The number of independant q-batches being run in parrallel to best make use of hardware.\n",
    "\n",
    "        Returns:\n",
    "            A `(b)`-dim Tensor of acquisition function values at the given\n",
    "            design points `X`. Larger values are considered better.\n",
    "\n",
    "\n",
    "        Botorch requirements:\n",
    "        - Requirement from optimisers:\n",
    "            Gradient: If `X.requires_grad = True` the output must support auto-differentiation from outputs to inputs.\n",
    "            The use of gradient is determined by the optimisation function applied to this function, and can not\n",
    "            be turned on/off from within the acquisition function (if turned on will be ignored, if turned off\n",
    "            ptimisation will throw error). Use of gradient though botorch is controlled with\n",
    "            `optimize_acqf(..., options = {\"with_grad\":False})`\n",
    "            See Notebook section \"Botorch Optimisation\" for details.\n",
    "\n",
    "        Ax requirement:\n",
    "        - Problem/Outcome space vs. Model space\n",
    "            - Ax shifts everything from problem-space to model-space at the ModelBridge level (e.g TorchModelBridge).\n",
    "            - As a result, the x value received are in model-space. Need to act accordingly.\n",
    "\n",
    "        Specific implementation:\n",
    "        - Provides a simple heuristic of optimising prblem with distributions by focusing on region with high\n",
    "          distribution probablity, and high GP uncertainty.\n",
    "        \"\"\"\n",
    "        assert X.shape[-1] == 1, \"Function only supports 1d input (d=1)\"\n",
    "\n",
    "        prbs = self.dist.log_prob(X).exp()\n",
    "        # was: (b,1,1), now (b,)\n",
    "        prbs = prbs.flatten()\n",
    "\n",
    "        posterior = self.model.posterior(X)\n",
    "        std = posterior.variance.sqrt().flatten()  # type: ignore\n",
    "\n",
    "        return prbs * std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_ac = CustomAcq(gp, DIST)\n",
    "X = torch.tensor([[[0.1]], [[0.5]], [[0.9]]])\n",
    "\n",
    "# Note: .model.posterior turns of gradient tracking by default. gradient behavior is detailed in the following section\n",
    "cust_ac(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Gradient behaviour\n",
    "Optimisation is controlled by `botorch.optim.optimize.optimize_acqf`, and gradient information is extracted in `botorch.generation.gen.gen_candidates_scipy`. The following mimics how gradient info is acquired in that function. This must produce the correct gradient for gradient based optimisation to be sucessful. Details covered in [Botroch Optimisation](#botroch-optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An acquisition function needs to be optimised with respect to its inputs.\n",
    "X = torch.tensor([[[0.1]]], requires_grad=True)\n",
    "# Is turned a maximisation to minimisation problem\n",
    "loss = -1 * cust_ac(X)\n",
    "_ = torch.autograd.grad(loss, X)\n",
    "\n",
    "\n",
    "## This can be used to show the grad graph\n",
    "# torchviz.make_dot(\n",
    "#     loss,\n",
    "#     dict(X = X),  # noqa: ERA001\n",
    "#     show_attrs=True,  # noqa: ERA001\n",
    "#     show_saved=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple points (t-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An acquisition function needs to be optimised with respect to its inputs.\n",
    "X = torch.tensor([[[0.1]], [[0.2]]], requires_grad=True)\n",
    "# Is turned a maximisation to minimisation problem\n",
    "loss = -1 * cust_ac(X)\n",
    "_ = torch.autograd.grad(loss.sum(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation of Acquisition:\n",
    "The acqusition function needs to be compatible with the optimiser that optimises it.\n",
    "- in `botorch`: `botorch.optim.optimize.optimize_acqf` is used to achieved this.\n",
    "- in `ax`: When a new point in requested from a Generator in the DoE loop it connects this to `botorch.optim.optimize.optimize_acqf`.\n",
    "\n",
    "The relationship between from `ax` (bottom), through `botorch`, to `scipy` (top) is described in the following stack trace:\n",
    "\n",
    "```sh\n",
    "minimize_with_timeout (botorch\\optim\\utils\\timeout.py:80) \n",
    "#   scipy.optimize.minimize  is called here\n",
    "gen_candidates_scipy (botorch\\generation\\gen.py:252)\n",
    "#   \"with_grad\" used here to determine optimisation approach:\n",
    "#       if True (line 192): \n",
    "#           - makes a wrapper around the acquisition function that give:  acq_wrapper(x) -> (funct_value, func_grad).\n",
    "#           - jac=True in scipy.optimize.minimize\n",
    "#       if False (line 224):\n",
    "#           - wrapper returns:  acq_wrapper(x) -> funct_value\n",
    "#           - jac=False in scipy.optimize.minimize\n",
    "_optimize_batch_candidates (botorch\\optim\\optimize.py:333)\n",
    "_optimize_acqf_batch (botorch\\optim\\optimize.py:349)\n",
    "_optimize_acqf (botorch\\optim\\optimize.py:584)\n",
    "optimize_acqf (botorch\\optim\\optimize.py:563)\n",
    "optimize (ax\\models\\torch\\botorch_modular\\acquisition.py:439)\n",
    "gen (ax\\models\\torch\\botorch_modular\\model.py:395)\n",
    "_gen (ax\\modelbridge\\torch.py:682)\n",
    "gen (ax\\modelbridge\\base.py:791)\n",
    "TorchModelBridge.gen()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force\n",
    "Finding the maximum of the acquisition function. Note, at this point, the optimal of the objective is a long way for the best part of the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.linspace(0, 1, 101)\n",
    "scores = cust_ac(test_x.reshape(-1, 1, 1))\n",
    "\n",
    "print(f\"Optimal values of acquisition: Best x {test_x[scores.argmax()]}. score {scores.max()}\")\n",
    "\n",
    "_ = plot_gp_and_dist(gp, acquisiton_function=cust_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Botroch Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to provide this with correct gradient information (in the tensor output object)\n",
    "# OR manually tell it to ignore gradient\n",
    "candidate, result = optimize_acqf(\n",
    "    cust_ac,\n",
    "    bounds=bounds,\n",
    "    q=1,\n",
    "    # This is how many different start location will be tried by the optimiser\n",
    "    num_restarts=5,\n",
    "    # TODO(sw 2024-11-13): confirm the impact of this paramerter\n",
    "    # Think this required when using a MCAcquisitionFunction (e.g the Acquisition function output at x is noisey).\n",
    "    # Controls how many time to repeat an x\n",
    "    raw_samples=100,\n",
    "    # Key parameter to control if optimisation should use gradient from the acquisiton fucntion\n",
    "    options={\"with_grad\": True},  # True by default\n",
    ")\n",
    "candidate, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstaration of lower level Botorch optimisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `optimize_acqf` internals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_conditions = gen_batch_initial_conditions(cust_ac, bounds, q=1, num_restarts=10, raw_samples=10)\n",
    "candidates, scores = gen_candidates_scipy(\n",
    "    initial_conditions=initial_conditions,\n",
    "    acquisition_function=cust_ac,\n",
    "    lower_bounds=torch.tensor([0.0]),\n",
    "    upper_bounds=torch.tensor([1.0]),\n",
    "    # NOTE: Optimisation fails when this is true\n",
    "    # if false does an optimisation that ignore the gradient\n",
    "    # Must mean that we are giveing it bad gradient info in one way or another\n",
    "    # options = dict(with_grad = False)  # noqa: ERA001\n",
    ")\n",
    "\n",
    "\n",
    "# get_best_candidates(candidates, scores)  # noqa: ERA001\n",
    "candidates[scores.argmax()], scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scipy direct optimisation\n",
    "Internally botorch evarually used this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make AcqusitionFunction compatible with scipy.optimisation:\n",
    "- turn it into a minimisation problem\n",
    "- use numpy\n",
    "\n",
    "This version doesn't use gradient information\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def acq_wrapper(x: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(x, dtype=torch.float64).reshape(-1, 1, 1)\n",
    "        acq_result = cust_ac(x)\n",
    "\n",
    "        # we are using a minimise function, so we need to flip the sign\n",
    "        return -1 * acq_result.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.minimize(acq_wrapper, x0=[0.3], method=\"SLSQP\", bounds=[(0.0, 1.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoE\n",
    "Perform a loop of DoE, where we use the gp and the acquisition function to give use the next datapoint for our acquisition function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Botorch\n",
    "#### Retraining the model at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(8, 20))\n",
    "\n",
    "bounds = torch.tensor([[0.0], [1.0]])  # 1D search space [lower_bound, upper_bound]\n",
    "train_x_opt = torch.tensor([[0.1], [0.85]])\n",
    "train_y_opt = underling_function(train_x_opt)\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    # Train the GP on the available data\n",
    "    gp = SingleTaskGP(train_X=train_x_opt, train_Y=train_y_opt)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    _ = fit_gpytorch_model(mll)\n",
    "    # Plot it and the acquisition function\n",
    "    cust_ac = CustomAcq(gp, DIST)\n",
    "\n",
    "    # estimate the best point in the objective function with the current model\n",
    "    gp_xmax_est = estimate_xmax_with_gp(gp)\n",
    "    test_str = f\"gp xmax est {gp_xmax_est:.2f}.\\nobjective_score {objective_func(gp_xmax_est):.2f}\"\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.05,\n",
    "        test_str,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment=\"bottom\",\n",
    "        bbox={\"boxstyle\": \"round,pad=0.3\", \"edgecolor\": \"black\", \"facecolor\": \"lightgray\"},\n",
    "    )\n",
    "\n",
    "    _ = plot_gp_and_dist(gp, acquisiton_function=cust_ac, ax=ax)\n",
    "\n",
    "    candidate, result = optimize_acqf(\n",
    "        cust_ac,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=5,\n",
    "        raw_samples=100,\n",
    "        # If can't provide grad information need to be able to manually set this\n",
    "        options={\"with_grad\": True},  # this is True by default\n",
    "    )\n",
    "\n",
    "    # call the simulator to give us the new results\n",
    "    train_x_opt = torch.cat([train_x_opt, candidate])\n",
    "    train_y_opt = torch.cat([train_y_opt, underling_function(candidate)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the same, but do not optimise the model hyperparams after the first fit\n",
    "condition on new data but don't update hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "bounds = torch.tensor([[0.0], [1.0]])  # 1D search space [lower_bound, upper_bound]\n",
    "train_x_opt = torch.tensor([[0.1], [0.85]])\n",
    "train_y_opt = underling_function(train_x_opt)\n",
    "\n",
    "\n",
    "gp = SingleTaskGP(train_X=train_x_opt, train_Y=train_y_opt)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "_ = fit_gpytorch_model(mll)\n",
    "\n",
    "for ax in axes:\n",
    "    # Plot it and the acquisition function\n",
    "    cust_ac = CustomAcq(gp, DIST)\n",
    "\n",
    "    # estimate the best point in the objective function with the current model\n",
    "    gp_xmax_est = estimate_xmax_with_gp(gp)\n",
    "    test_str = f\"gp xmax est {gp_xmax_est:.2f}.\\nobjective_score {objective_func(gp_xmax_est):.2f}\"\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.05,\n",
    "        test_str,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment=\"bottom\",\n",
    "        bbox={\"boxstyle\": \"round,pad=0.3\", \"edgecolor\": \"black\", \"facecolor\": \"lightgray\"},\n",
    "    )\n",
    "\n",
    "    _ = plot_gp_and_dist(gp, acquisiton_function=cust_ac, ax=ax)\n",
    "\n",
    "    candidate, result = optimize_acqf(\n",
    "        cust_ac,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=5,\n",
    "        raw_samples=100,\n",
    "        # If can't provide grad information need to be able to manually set this\n",
    "        options={\"with_grad\": True},  # This is true by default\n",
    "    )\n",
    "\n",
    "    # Update the model\n",
    "    new_x_point = candidate.flatten()\n",
    "    new_y_point = underling_function(new_x_point)\n",
    "    gp = gp.get_fantasy_model(inputs=new_x_point, targets=new_y_point)\n",
    "\n",
    "    # Justed used for plotting now\n",
    "    train_x_opt = torch.cat([train_x_opt, new_x_point.unsqueeze(0)])\n",
    "    train_y_opt = torch.cat([train_y_opt, new_y_point.unsqueeze(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AX\n",
    "Put this into the ax framework and optimise it that way.\n",
    "- Come from botorch custom acquisitions [here.](https://botorch.org/tutorials/custom_acquisition)\n",
    "- and Ax example here [here](https://ax.dev/tutorials/modular_botax.html)\n",
    "\n",
    "There are likely otherways to acheive this with the ax interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our purposes, the metric is a wrapper that structures the function output.\n",
    "class DummyMetric(Metric):  # noqa: D101\n",
    "    def fetch_trial_data(self, trial):  # type: ignore  # noqa: ANN001, D102\n",
    "        records = []\n",
    "        for arm_name, arm in trial.arms_by_name.items():\n",
    "            params = arm.parameters\n",
    "            tensor_params = torch.tensor([params[\"x\"]])\n",
    "            records.append(\n",
    "                {\n",
    "                    \"arm_name\": arm_name,\n",
    "                    \"metric_name\": self.name,\n",
    "                    \"trial_index\": trial.index,\n",
    "                    \"mean\": underling_function(tensor_params),\n",
    "                    \"sem\": float(\"nan\"),  # SEM (observation noise) - NaN indicates unknown\n",
    "                }\n",
    "            )\n",
    "        # OK just wtapps saying it contains a sucessful value\n",
    "        return Ok(value=Data(df=pd.DataFrame.from_records(records)))\n",
    "\n",
    "\n",
    "# Search space defines the parameters, their types, and acceptable values.\n",
    "search_space = SearchSpace(\n",
    "    parameters=[\n",
    "        RangeParameter(name=\"x\", parameter_type=ParameterType.FLOAT, lower=0, upper=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "optimization_config = OptimizationConfig(\n",
    "    objective=Objective(\n",
    "        # NOTE: we don't care about lower_is_better because are not just trying to maximise or minimise this value\n",
    "        # This just needs to be the y value we want to fit our GP to\n",
    "        metric=DummyMetric(name=\"dummy_metric\", lower_is_better=True),\n",
    "        minimize=True,  # This is optional since we specified `lower_is_better=True`\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "class MyRunner(Runner):  # noqa: D101\n",
    "    def run(self, trial):  # type: ignore  # noqa: ANN001, D102\n",
    "        trial_metadata = {\"name\": str(trial.index)}\n",
    "        return trial_metadata\n",
    "\n",
    "\n",
    "exp = Experiment(\n",
    "    name=\"Dummy_experiment\",\n",
    "    search_space=search_space,\n",
    "    optimization_config=optimization_config,\n",
    "    runner=MyRunner(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the input constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super hacky way of making sure the input constructure is not already in there,\n",
    "# so we can make updates to this functions without having to restart the notebook\n",
    "input_constructors.ACQF_INPUT_CONSTRUCTOR_REGISTRY.pop(CustomAcq, None)\n",
    "\n",
    "\n",
    "@acqf_input_constructor(CustomAcq)\n",
    "def construct_inputs_custom_acq(\n",
    "    model: Model,\n",
    "    dist: Distribution,\n",
    "    **kwargs: Any,  # noqa: ANN401, ARG001\n",
    ") -> dict[str, Any]:\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"dist\": dist,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: low level Ax\n",
    "Note, with this hyperparams are refitted every round of DoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually add some data to the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_exp_locations = [[0.1], [0.85]]\n",
    "for x in manual_exp_locations:\n",
    "    # this is just a quick and diry unpacking for now, will need to extend to x in higher dims\n",
    "    trial_arm = exp.new_trial().add_arm(Arm(parameters={\"x\": x[0]}))\n",
    "    trial_arm.run()\n",
    "    trial_arm.mark_completed()\n",
    "\n",
    "# Check this has been properly attached\n",
    "_ = display(exp.fetch_data().df)\n",
    "exp.trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop build a new GP from the available data, and use the custom acquisition function to select a new point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    model_bridge_cust_ac = Models.BOTORCH_MODULAR(\n",
    "        experiment=exp,\n",
    "        data=exp.fetch_data(),\n",
    "        # surrogate=Surrogate(FixedNoiseGP),  # Optional, will use default if unspecified  # noqa: ERA001\n",
    "        botorch_acqf_class=CustomAcq,  # Optional, will use default if unspecified\n",
    "        acquisition_options={\"dist\": DIST},\n",
    "    )\n",
    "    trial = exp.new_trial(\n",
    "        generator_run=model_bridge_cust_ac.gen(\n",
    "            1,\n",
    "            # This section is optional. with_grad default to True if not specified here\n",
    "            model_gen_options={\n",
    "                \"optimizer_kwargs\": {\n",
    "                    \"options\": {\"with_grad\": True},\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    _ = trial.run()\n",
    "    _ = trial.mark_completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the resulting experiment object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.fetch_data().df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(\n",
    "    plot_slice(\n",
    "        model=model_bridge_cust_ac,\n",
    "        param_name=\"x\",\n",
    "        metric_name=\"dummy_metric\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this produces the same points as the botorch model that is fully retrained at each step, as expected.\n",
    "\n",
    "- TODO(sw 2024-11-13): Build the equivalent plot to the botorch component using the Ax interface to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Higher level Ax: Generation Stratergy\n",
    "Uses higher level ax services. Note, with this hyperparams are refitted every round of DoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the experiment fresh\n",
    "exp = Experiment(\n",
    "    name=\"Dummy_experiment\",\n",
    "    search_space=search_space,\n",
    "    optimization_config=optimization_config,\n",
    "    runner=MyRunner(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GenerationStrategy(\n",
    "    steps=[\n",
    "        # Quasi-random initialization step\n",
    "        GenerationStep(\n",
    "            model=Models.SOBOL,\n",
    "            num_trials=2,  # How many trials should be produced from this generation step\n",
    "            model_kwargs={\"seed\": 999},  # Any kwargs you want passed into the model\n",
    "        ),\n",
    "        # Bayesian optimization step using the custom acquisition function\n",
    "        GenerationStep(\n",
    "            model=Models.BOTORCH_MODULAR,\n",
    "            num_trials=-1,  # No limitation on how many trials should be produced from this step\n",
    "            # For `BOTORCH_MODULAR`, we pass in kwargs to specify what surrogate or acquisition function to use.\n",
    "            # `acquisition_options` specifies the set of additional arguments to pass into the input constructor.\n",
    "            model_kwargs={\n",
    "                \"botorch_acqf_class\": CustomAcq,\n",
    "                \"acquisition_options\": {\"dist\": DIST},\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.ax_client import AxClient\n",
    "\n",
    "# Initialize the client - AxClient offers a convenient API to control the experiment\n",
    "ax_client = AxClient(generation_strategy=gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet supported ax_client.load_experiment('Dummy_experiment')\n",
    "\n",
    "# Bit of a hacky way to attach it to the higher level thing\n",
    "_ = ax_client._set_runner(experiment=exp)  # noqa: SLF001\n",
    "_ = ax_client._set_experiment(  # noqa: SLF001\n",
    "    experiment=exp,\n",
    "    overwrite_existing_experiment=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE we have already defined the objective inside the experiement, annoying we need to define it again\n",
    "def evaluate(parameters):  # type: ignore  # noqa: ANN001\n",
    "    # Note this is just because our objective function works with tensors\n",
    "    x = torch.tensor([parameters.get(\"x\")], dtype=torch.float64)\n",
    "    # In our case, standard error is 0, since we are computing a synthetic function.\n",
    "    return {\"dummy_metric\": (underling_function(x).item(), 0.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Running trial {i+1}/30...\")\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(\n",
    "    plot_slice(\n",
    "        model=ax_client.generation_strategy.model,\n",
    "        param_name=\"x\",\n",
    "        metric_name=\"dummy_metric\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3:  Alternate way of interacting with the model\n",
    "- create a botorch model useing the interface `ax.models.torch.botorch` [here](https://ax.dev/api/models.html#module-ax.models.torch.botorch)\n",
    "- also a relevant tutorial [here](https://botorch.org/tutorials/custom_botorch_model_in_ax)\n",
    "\n",
    "TODO: Explore this further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
