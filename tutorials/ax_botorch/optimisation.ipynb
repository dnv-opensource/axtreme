{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "Optimisation plays an important role in the DoE process. It is important that the optimisation settings are appropriate for acquisition being optimised. If the acquisition function uses a QoI, the QoI behavior is also important.\n",
    "\n",
    "The following acquisition characteristic impact the optimiser settings required.\n",
    "- Stochasticity\n",
    "- Smoothness (type of gradients available).\n",
    "\n",
    "For each characteristic this notebook covers:\n",
    "- How to assess your acqusition function for this characteristic.\n",
    "- Optimisation implications\n",
    "\n",
    "Additionally this notebook contains:\n",
    "- Glossary of important optimiser setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "from axtreme.acquisition.qoi_look_ahead import QoILookAhead\n",
    "from axtreme.data import BatchInvariantSampler2d, MinimalDataset, SizableSequentialSampler\n",
    "from axtreme.plotting.gp_fit import plot_1d_model\n",
    "from axtreme.qoi import GPBruteForce\n",
    "from axtreme.sampling import NormalIndependentSampler\n",
    "from axtreme.utils.gradient import is_smooth_1d\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Example acquisition function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:\n",
    "NOTE: this is the exact model defined in `tests/conftest.py: model_singletaskgp_d1_t2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target1(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sin(x * 10)\n",
    "\n",
    "\n",
    "def target2(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * torch.cos(x * 10) + 0.8\n",
    "\n",
    "\n",
    "def y_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.concat([target1(x), target2(x)], dim=-1)\n",
    "\n",
    "\n",
    "train_x = torch.tensor([[0.1], [0.5], [0.9], [1.0]])\n",
    "train_y = y_func(train_x)\n",
    "\n",
    "model = SingleTaskGP(train_X=train_x, train_Y=train_y, train_Yvar=torch.ones_like(train_y) * 0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "_ = fit_gpytorch_mll(mll)\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "x = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "_ = ax.plot(x, target1(x), label=\"target0\")\n",
    "_ = ax.plot(x, target2(x), label=\"target1\")\n",
    "_ = plot_1d_model(model, ax=ax)\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an example acquisition function\n",
    "This is a smaller version of the typical Acquisiton/QoI combination we use. It freezes all sources of randomness to ensure it is deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_periods = 3\n",
    "N_ENV_SAMPLES_PER_PERIOD = 1000\n",
    "\n",
    "rng = np.random.default_rng(1337)\n",
    "\n",
    "env_data = rng.normal(size=10_000) / 3 + 0.5\n",
    "env_data = env_data[(env_data < 1) & (env_data > 0)]\n",
    "env_data = env_data.reshape(-1, 1)  # Make it compatible\n",
    "dataset = MinimalDataset(env_data)\n",
    "\n",
    "gen = torch.Generator()\n",
    "_ = gen.manual_seed(7)\n",
    "sampler = RandomSampler(dataset, num_samples=n_periods * N_ENV_SAMPLES_PER_PERIOD, generator=gen, replacement=True)\n",
    "\n",
    "\n",
    "sampler = SizableSequentialSampler(dataset, n_periods * N_ENV_SAMPLES_PER_PERIOD)\n",
    "\n",
    "batch_sampler = BatchInvariantSampler2d(\n",
    "    sampler=sampler,\n",
    "    batch_shape=torch.Size([n_periods, 256]),\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "qoi = GPBruteForce(\n",
    "    env_iterable=dataloader,\n",
    "    erd_samples_per_period=1,\n",
    "    posterior_sampler=NormalIndependentSampler(torch.Size([10]), seed=14),\n",
    "    # When true, All uncertainty shown in estimates is due to uncertainty in the surrogate model.\n",
    "    shared_surrogate_base_samples=True,\n",
    "    no_grad=True,\n",
    "    seed=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acqusition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acqf = QoILookAhead(model, qoi_estimator=qoi)\n",
    "\n",
    "x_domain = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "acqf_scores = acqf(x_domain.unsqueeze(-1))\n",
    "print(f\"simple brute force max: score {acqf_scores.max().item()} at {x_domain[acqf_scores.argmax()].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(x_domain.flatten(), acqf_scores)\n",
    "_ = plt.title(\"Acqusition function surface (brute force)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chracteristic: Stochastic Output\n",
    "Acquisition which have stochastic output (randomness in the score they return) are not yet supported. `axtreme` assumes that the acquisition function is deterministic.\n",
    "\n",
    "Note: Botorch support stochastic acquisition function. See `MCAcquisitionFunction` for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Assessing Stochasticity\n",
    "A simple (although not conclusive) way to sanity check if your acquisition function is stochastic is to run it multiple times for the same input. If the results differ, the function is stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_domain = torch.linspace(0, 1, 10).reshape(-1, 1)\n",
    "acqf_scores1 = acqf(x_domain.unsqueeze(-1))\n",
    "acqf_scores2 = acqf(x_domain.unsqueeze(-1))\n",
    "\n",
    "torch.testing.assert_close(acqf_scores1, acqf_scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristic: Smoothness\n",
    "Smoothness (specifically, being once or twice differentiable) is an important property because it determines which optimisers can be used. Gradient based optimisation is typically more effecient, but requires smooth function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing smoothness\n",
    "A simple (although not conclusive) way to sanity check if your acquisition function is smooth is to run it for small changes in x and check the acquisition function remains smooth. We make use of a helper to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_domain = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "acqf_scores = acqf(x_domain.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = is_smooth_1d(x_domain.flatten(), acqf_scores, plot=True, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests to check for 1st and 2nd degree smoothness.\n",
    "\n",
    "NOTE: the threshold values may need to be adjusted for your specific problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = is_smooth_1d(x_domain.flatten(), acqf_scores, d1_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation Implications.\n",
    "#### 0 times differentiable.\n",
    "\"Nelder-Mead\" or other gradient free methods should be used. These can require a larger number of acquisition function runs, might be worth adjusting `maxiter`\n",
    "\n",
    "```python\n",
    "optimize_acqf(\n",
    "    ...\n",
    "    options = {\n",
    "        \"methed\": \"Nelder-Mead\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "#### 1 time differentiable\n",
    "As above.\n",
    "\n",
    "#### 2 times differentiable\n",
    "Second order optimisation algorithems such as \"L-BFGS-B\" can be used\n",
    "\n",
    "e.g\n",
    "```python\n",
    "optimize_acqf(\n",
    "    ...\n",
    "    options = {\n",
    "        \"methed\": \"L-BFGS-B\"\n",
    "    }\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Nelder-Mead setting for this specific problem.\n",
    "#### **1: Default params:**\n",
    "Use default \"Nelder-Mead\" params. Note: all the params here differ from ax defaults.\n",
    "```python\n",
    "optimize_acqf(\n",
    "    ...,\n",
    "    num_restarts=6,\n",
    "    raw_samples=10,\n",
    "    options={\n",
    "        \"with_grad\": False,  # True by default.\n",
    "        \"method\": \"Nelder-Mead\",  # \"L-BFGS-B\" by default\n",
    "    },\n",
    ")\n",
    "```\n",
    "- runtime: 2:20 mins\n",
    "- result: basically perfect\n",
    "\n",
    "#### **2: brute force**\n",
    "- 100 runs over linspace(0,1):\n",
    "- runtime: 3 seconds\n",
    "- function eval: 100\n",
    "\n",
    "#### **3: Limit the maxiter or max functio neval**\n",
    "- By default perfroms 200 maxiter and ~400 feval are used. Can reducing this give a suitably accurate result?\n",
    "\n",
    "\n",
    "```python\n",
    "optimize_acqf(\n",
    "    ...,\n",
    "    num_restarts=6,\n",
    "    raw_samples=10,\n",
    "    options={\n",
    "        \"with_grad\": False,  # True by default.\n",
    "        \"method\": \"Nelder-Mead\",  # \"L-BFGS-B\" by default\n",
    "        \"maxfev\": 50, \n",
    "    },\n",
    "    retry_on_optimization_warning= False\n",
    ")\n",
    "```\n",
    "- runtime: 15 seconds, good results\n",
    "- function eval: 10 + 6 * 50 = 310\n",
    "\n",
    "\n",
    "\n",
    "#### **4: Best of random samples (skip opt)**\n",
    "intuition: the brute force was fast and good enough. Can we do something equivalent trhough botorch.\n",
    "```python\n",
    "num_restarts=50,\n",
    "raw_samples=50,\n",
    "options={\n",
    "    \"with_grad\": False,  # True by default.\n",
    "    \"method\": \"Nelder-Mead\",  # \"L-BFGS-B\" by default\n",
    "    \"maxfev\": 0, # this raises warning and it then retires unless set\n",
    "    \"batch_size\": 50\n",
    "},\n",
    "retry_on_optimization_warning= False\n",
    "```\n",
    "- runtime: 5 seconds 50 pts\n",
    "- runtime: 10 seconds with 100 pts\n",
    "- function eval: 50 + 50 * 0 = 50\n",
    "\n",
    "\n",
    "#### **4: Mix of points and optimisation**\n",
    "```python\n",
    "candidate, result = optimize_acqf(\n",
    "   ...\n",
    "    num_restarts=20,\n",
    "    raw_samples=20,\n",
    "    options={\n",
    "        \"with_grad\": False,  # True by default.\n",
    "        \"method\": \"Nelder-Mead\",  # \"L-BFGS-B\" by default\n",
    "        #\"fatol\": .1\n",
    "        \"maxfev\": 1, # this raises warning and it then retires unless set\n",
    "        \"batch_size\":10,\n",
    "        \"init_batch_size\": 20\n",
    "    },\n",
    "    retry_on_optimization_warning= False\n",
    ")\n",
    "```\n",
    "- runtime: 3 seconds. Results start to get a bit rough\n",
    "- function eval: 20 + 1 * 20 = 40\n",
    "- NOTE: results starting to get a little rough (still in a good region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate, result = optimize_acqf(\n",
    "    acqf,\n",
    "    bounds=torch.tensor([[0.0], [1.0]]),\n",
    "    q=1,\n",
    "    num_restarts=6,\n",
    "    raw_samples=10,\n",
    "    options={\n",
    "        \"with_grad\": False,  # True by default.\n",
    "        \"method\": \"Nelder-Mead\",  # \"L-BFGS-B\" by default\n",
    "    },\n",
    ")\n",
    "candidate, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These setting give us a suitable accuracy and runtime on this problem.\n",
    "\n",
    "NOTE: When scaling to different problem consider adjusting the following params.\n",
    "- maxfev: scipy default is 200 * n_dim in x.\n",
    "- num_restarts: Note - ax does not change this depending on the problem.\n",
    "- raw_samples: Note - ax does not change this depending on the problem.\n",
    "\"\"\"\n",
    "candidate, result = optimize_acqf(\n",
    "    acqf,\n",
    "    bounds=torch.tensor([[0.0], [1.0]]),\n",
    "    q=1,\n",
    "    num_restarts=30,\n",
    "    raw_samples=30,\n",
    "    options={\n",
    "        \"with_grad\": False,  # True by default.\n",
    "        \"method\": \"Nelder-Mead\",  # \"L-BFGS-B\" by default\n",
    "        \"maxfev\": 4,  # this raises warning and it then retires unless set\n",
    "        \"batch_size\": 10,\n",
    "    },\n",
    "    retry_on_optimization_warning=False,\n",
    ")\n",
    "\n",
    "candidate, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser Settings.\n",
    "In this section we focus on the optimisation arguementz for the botorch function `optimize_acqf`. This is called by ax, and internally calls scipy (see [optimisation path](#optimisation-path) for details).\n",
    "\n",
    "### `optimize_acqf`\n",
    "The documentation for this function is found [here](https://botorch.org/api/optim.html#botorch.optim.optimize.optimize_acqf). The following provides additional imformation not listed in this documentation:\n",
    "\n",
    "Basic behaviour: uses `gen_batch_initial_conditions` (or a custom method/points provided by user) to determine the initial points the optimisation should be tried from, then uses `scipy.optimize.minimize` to optimise those points.\n",
    "\n",
    "Arguements:\n",
    "- `acq_function`: See docs.\n",
    "- `bounds`: See docs.\n",
    "- `q`: See docs.\n",
    "- `num_restarts`:\n",
    "    - The number of initial points that the optimiser will start from (e.g how many different starting location will be used). This is used because the optimisation is typically non-convex/flat. Each restart is like an optimisation in local region.\n",
    "    - default passed by ax: 20.\n",
    "        - set in `ax\\models\\torch\\botorch_modular\\acquisition.py 418` using `ax\\models\\torch\\botorch_modular\\optimizer_argparse.py 33`\n",
    "- `raw_samples`:\n",
    "    - see [gen_batch_initial_conditions](#gen_batch_initial_conditions) section below.\n",
    "    - default passed by ax: 1024.\n",
    "        - set in `ax\\models\\torch\\botorch_modular\\acquisition.py 418` using `ax\\models\\torch\\botorch_modular\\optimizer_argparse.py 33`\n",
    "- `options`: This is a dictionary of key value pairs\n",
    "    - `nonnegative=False`: Set to true if the acquisition score can never be negative.\n",
    "        - This is used in `gen_batch_initial_conditions` to help pick better inital points.\n",
    "    - `init_batch_limit: int`: \n",
    "        - This is used in `gen_batch_initial_conditions`. See section for details. \n",
    "        - default passed by ax: 32.\n",
    "            - set in `ax\\models\\torch\\botorch_modular\\acquisition.py 418` using `ax\\models\\torch\\botorch_modular\\optimizer_argparse.py 33`\n",
    "    - `batch_limit`: int: controls how manch points are optimiseed at once in scipy (how many of the total `num_restart` points to do at once/in a single batch)\n",
    "        - used in `botorch\\optim\\optimize.py 294`\n",
    "        - default passed by ax: 5.\n",
    "            - set in `ax\\models\\torch\\botorch_modular\\acquisition.py 418` using `ax\\models\\torch\\botorch_modular\\optimizer_argparse.py 33`\n",
    "    - `with_grad=True`: If true, automatic differentiation (`torch.autograd.grad(loss,x)` of the scores w.r.t the input x's must be supported. This gradient is then provided directly to Scipy optimise.\n",
    "        - NOTE: This does NOT control if gradient based optimisation is performed by Scipy (e.g Some Scipy method may internally estimate gradient if `with_grad=False`).\n",
    "    - `method`: This is passed to `method` arguement of `scipy.optimize.minimize`. See [section](#scipyoptimizeminimize) for details.\n",
    "        - `\"L-BFGS-B\"`: used by default if there are no constraints (`equality_constraints`, `inequality_constraints`, `nonlinear_inequality_constraints`)\n",
    "        - `SLSQP`: used if constraints are present.\n",
    "    - `maxiter=2000`: passed to `maxiter` options arguement in `scipy.optimize.minimize`. The max number of iterations of optimisation to perform.\n",
    "        - a single iteration updates all points (in `batch_limit`) as the problem is posed to scipy as joint problem.\n",
    "        - as per scipy \"each iteration may use several function evaluations.\"\n",
    "    - Additional args: specific to the method being used can be passed here.\n",
    "- `inequality_constraints`: See docs.\n",
    "- `equality_constraints`: See docs.\n",
    "- `nonlinear_inequality_constraints`: See docs.\n",
    "- `fixed_features`: See docs.\n",
    "- `post_processing_func`: See docs.\n",
    "- `batch_initial_conditions`: if provided, skips using `gen_batch_initial_conditions` internally, and uses these samples directly.\n",
    "    - Should be of shape `num_restarts x q x d`.\n",
    "- `return_best_only: = True`: If false, will return the optimisation results for the `num_restarts` rounds.\n",
    "- `gen_candidates`: method to use internally instead of `gen_batch_initial_conditions`.\n",
    "- `sequential=False`: if q > 1, optimise sequentially or jointly.\n",
    "- `ic_generator`: See docs.\n",
    "- `timeout_sec`: If exceeded, the `scipy.optimize.minimize` will be stopped, and returns the current state/progress of the optimiser, similar to when `maxiter` is reached.\n",
    "- `return_full_tree`: No docs found.\n",
    "- `retry_on_optimization_warning`: See docs.\n",
    "- `**ic_gen_kwargs`: See docs.\n",
    "\n",
    "#### `gen_batch_initial_conditions`\n",
    "located here: `botorch\\optim\\initializers.py:423, in gen_batch_initial_conditions`\n",
    "##### basic functionality\n",
    "- first generate `raw_samples` many random x points.\n",
    "- evaluates each one with the acquisition function.\n",
    "- Uses a heuristic to pick `num_restart` of the `raw_sample` points (the function use the x location and the score from the acquisition function) \n",
    "\n",
    "##### Args:\n",
    "- `raw_samples`: See basic functionality. The larger this number, the more times the acquisition function is run before Scipy. Majority of these will be discarded.\n",
    "- `init_batch_limit`: How many of the `raw_samples` to pass to the acquisition function at once when scoring them. Decrease if acquisition function is having memory issues.\n",
    "\n",
    "#### `scipy.optimize.minimize`\n",
    "\n",
    "Args:\n",
    "- `method=str`:\n",
    "    - `\"Nelder-Mead\"`: Robust algorithem requiring no derivatives\n",
    "    - `\"L-BFGS-B\"`: \n",
    "        - If gradient is not passed, will approximate gradient internally using finite difference (as per [here](https://stackoverflow.com/questions/18985048/error-using-l-bfgs-b-in-scipy)). \n",
    "        - Second order optimisation algo which approximates the hessian.\n",
    "        - Acquisition function should be smooth (twice differentiable).\n",
    "- \n",
    "\n",
    "### Optimisation path\n",
    "The optimisation relationship from `ax` (bottom), through `botorch`, to `scipy` (top) is described in the following stack trace (with notes describing some key points):\n",
    "\n",
    "```sh\n",
    "minimize_with_timeout (botorch\\optim\\utils\\timeout.py:80) \n",
    "#   scipy.optimize.minimize  is called here\n",
    "gen_candidates_scipy (botorch\\generation\\gen.py:252)\n",
    "#   \"with_grad\" used here to determine optimisation approach:\n",
    "#       if True (line 192): \n",
    "#           - makes a wrapper around the acquisition function that give:  acq_wrapper(x) -> (funct_value, func_grad).\n",
    "#           - jac=True in scipy.optimize.minimize\n",
    "#       if False (line 224):\n",
    "#           - wrapper returns:  acq_wrapper(x) -> funct_value\n",
    "#           - jac=False in scipy.optimize.minimize\n",
    "_optimize_batch_candidates (botorch\\optim\\optimize.py:333)\n",
    "_optimize_acqf_batch (botorch\\optim\\optimize.py:349)\n",
    "_optimize_acqf (botorch\\optim\\optimize.py:584)\n",
    "optimize_acqf (botorch\\optim\\optimize.py:563)\n",
    "optimize (ax\\models\\torch\\botorch_modular\\acquisition.py:439)\n",
    "gen (ax\\models\\torch\\botorch_modular\\model.py:395)\n",
    "_gen (ax\\modelbridge\\torch.py:682)\n",
    "gen (ax\\modelbridge\\base.py:791)\n",
    "TorchModelBridge.gen()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
